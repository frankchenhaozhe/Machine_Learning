{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ekjsmds4_n2G",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Problem Set 6 (Total points: 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VxmzV7JUbHjx"
   },
   "source": [
    "### Q1. Markov Decision Process [20 points]\n",
    "Imagine an MDP corresponding to playing slot machines in a casino. Suppose you start with \\$20\\$ cash to spend in the casino,  and you decide to play until you lose all your money or until you double your money (i.e., increase your cash to at least \\$40\\$). There are two slot machines you can choose to play: 1) slot machine X costs \\$10\\$ to play and will pay out \\$20\\$ with probability \\$0.05\\$ and will pay \\$0\\$ otherwise;\n",
    "and \\$2)\\$ slot machine Y costs \\$20\\$ to play and will pay out \\$30\\$ with probability 0.01 and \\$0 \\$ otherwise. As you are playing, you keep choosing machine X or Y at each turn.\n",
    "\n",
    "Write down the MDP that corresponds to the above problem. Clearly specify the state space, action space, rewards and transition probabilities. Indicate which state(s) are terminal. Assume that the discount factor γ = 1.\n",
    "\n",
    "**Notes:** There are several valid ways to specify the MDP, so you have some flexibility in your solution. For example, rewards can take many different forms, but overall you should get a higher reward for stopping when you double your money than when you lose all your money!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "States $S_i$ where $i \\in \\{0, 10, 20, 30, 40\\}$ and $s_0$ and $s_{40}$ are terminal\n",
    "\n",
    "Actions $A = \\{X, Y\\}$ denotes the slot machine we decide to play with.\n",
    "\n",
    "Rewards: $R(s_i, a, s_j) = j - i $\n",
    "\n",
    "Rewards: $R(s_i, a, s_0) = 1 $ if $s_0 == s_{40}$, else 0, would also work. \n",
    "\n",
    "Transition probabilities(there are several valid ways to define these):\n",
    "\n",
    "$P(s_{i-10}|s_i, X) = 0.95$\n",
    "\n",
    "$P(s_{i+10}|s_i, X) = 0.05$\n",
    "\n",
    "$P(s_{i-20}|s_i, Y) = 0.99$\n",
    "\n",
    "$P(s_{i+10}|s_i, Y) = 0.01$\n",
    "\n",
    "The above transitions are only valid for $X$ in states $s_{10}, s_{20}, s_{30}$, and for $Y$ in states $s_{20}, s_{30}$. All other transitions do not exist(have probability 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M_H3z6wR_n2I"
   },
   "source": [
    "## Reinforcement Learning\n",
    "In the remainder of the problem set you will implement the Q-Learning Algorithm to solve the \"Frozen Lake\" problem.\n",
    "We​ ​will​ ​use​ ​OpenAI’s​ ​gym​ ​package​ ​to​ ​develop​ ​our​ ​solution​ ​in​ ​Python.​ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q8sgYT6oUKGz"
   },
   "source": [
    "### OpenAI Gym Setup \n",
    "​Read the​ ​set-up​ ​instructions for​ ​Gym​  [here](https://gym.openai.com/docs/).​ ​The​ ​instructions​ ​also​ ​give​ ​a​ ​good​ ​overview​ ​of​ ​the​ ​API​ ​for​ ​this​ ​package,​ ​so​ ​please​ ​read through​ ​it​ ​before​ ​proceeding.​\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pQ-qqj1n_n2K",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Frozen Lake\n",
    "Instead​ ​of​ ​using​ ​CartPole,​ ​we’re​ ​going​ ​to​ ​be​ ​using​ ​the​ [​​FrozenLake](https://gym.openai.com/envs/FrozenLake-v0/) environment. Read through the code for this environment by following the Github link.​ <br>\n",
    "\n",
    "Winter​ ​is​ ​quickly​ ​approaching,​ ​and​ ​we​ ​have​ ​to​ ​worry​ ​about​ ​navigating​ ​frozen​ ​lakes.​ ​It’s​ ​only early November,​ ​\n",
    "so​ ​the​ ​lakes​ ​haven’t​ ​completely​ ​frozen​ ​and​ ​if​ ​you​ ​make​ ​the​ ​wrong​ ​step​ ​you​ ​may​ ​fall​ ​through. \n",
    "We’ll​ ​need​ ​to​ ​learn​ ​how​ ​to​ ​get​ ​to​ ​our​ ​destination​ ​when​ ​stuck​ ​on​ ​the​ ​ice,​ ​without​ ​falling​ ​in.\n",
    "The​ ​lake​ ​we’re​ ​going​ ​to​ ​consider​ ​is a​ ​square​ ​lake​ ​with​ ​spots​ ​to​ ​step​ ​on​ ​in​ ​the​ ​shape​ ​of​ ​a​ ​grid.​ ​​<br>\n",
    "\n",
    "The surface is described using a 4x4 grid like the following\n",
    "\n",
    "        S F F F \n",
    "        F H F H\n",
    "        F F F H\n",
    "        H F F G\n",
    "\n",
    "​Each​ ​spot​ ​can have​ ​one​ ​of​ ​four​ ​states:\n",
    "- S:​ ​starting​ ​point.\n",
    "- G:​ ​goal​ ​point.\n",
    "- F:​ ​frozen​ ​spot,​ ​where​ ​it’s​ ​safe​ ​to​ ​walk.\n",
    "- H:​ ​hole​ ​in​ ​the​ ​ice,​ ​where​ ​it’s​ ​not​ ​safe​ ​to​ ​walk.<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4fPxEMPQ_n2L"
   },
   "source": [
    "For example, consider the lake, <br>\n",
    "![alt text](https://cs-people.bu.edu/sunxm/frozen_lake_example.png)\n",
    "\n",
    "There are four possible actions: UP, DOWN, LEFT, RIGHT. Although we​ ​can​ ​see​ ​the​ ​path​ ​we​ ​need​ ​to​ ​walk,​ the agent does not. ​We’re​ ​going​ ​to​ ​train​ ​an​ ​agent​ ​to​ ​discover​ ​this​ ​via​ ​problem solving.​ ​However,​ ​walking​ ​on​ ​ice​ ​isn’t​ ​so​ ​easy!​ ​Sometimes​ ​you​ ​slip​ ​and​ ​aren’t​ ​able​ ​to​ ​take​ ​the​ ​step​ ​you intended.\n",
    "\n",
    "The episode ends when you reach the goal or fall in a hole.\n",
    "\n",
    "You receive a reward of 1 if you reach the goal, and zero otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3ZlKYHn-XE2X"
   },
   "source": [
    "#### Q2. Walking on the Frozen Lake [10 points]\n",
    "\n",
    "Write a script that sets up the Frozen Lake environment and takes 10 walks through it, consisting of a maximum 10 randomly sampled actions during each walk. After each step, render the current state, and print out the reward and whether the walk is \"done\", i.e. in the terminal state, because the agent fell into a hole (stop if it is). In your own words, explain how this environment behaves. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The environment behaves in a stochastic way, i.e. taking an action does not deterministically transition the agent into the next state. Instead the agent ends up in one of serveral states with some probability. The analogy here is that when you try to walk right on ice, you may slip and end up walking forward, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym in /opt/anaconda3/lib/python3.7/site-packages (0.17.3)\n",
      "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /opt/anaconda3/lib/python3.7/site-packages (from gym) (1.5.0)\n",
      "Requirement already satisfied: scipy in /opt/anaconda3/lib/python3.7/site-packages (from gym) (1.3.1)\n",
      "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /opt/anaconda3/lib/python3.7/site-packages (from gym) (1.2.2)\n",
      "Requirement already satisfied: numpy>=1.10.4 in /opt/anaconda3/lib/python3.7/site-packages (from gym) (1.17.2)\n",
      "Requirement already satisfied: future in /opt/anaconda3/lib/python3.7/site-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.17.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install gym\n",
    "\n",
    "import gym,sys,numpy as np\n",
    "from gym.envs.registration import register"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1 : Up\n",
      "  (Up)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "current state: 0\n",
      "reward 0.0\n",
      "done? False\n",
      "step 2 : Down\n",
      "  (Down)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "current state: 4\n",
      "reward 0.0\n",
      "done? False\n",
      "step 3 : Right\n",
      "  (Right)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "current state: 0\n",
      "reward 0.0\n",
      "done? False\n",
      "step 4 : Down\n",
      "  (Down)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "current state: 1\n",
      "reward 0.0\n",
      "done? False\n",
      "step 5 : Down\n",
      "  (Down)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "current state: 1\n",
      "reward 0.0\n",
      "done? False\n",
      "step 6 : Up\n",
      "  (Up)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "current state: 1\n",
      "reward 0.0\n",
      "done? False\n",
      "step 7 : Up\n",
      "  (Up)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "current state: 0\n",
      "reward 0.0\n",
      "done? False\n",
      "step 8 : Down\n",
      "  (Down)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "current state: 4\n",
      "reward 0.0\n",
      "done? False\n",
      "step 9 : Down\n",
      "  (Down)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "current state: 1\n",
      "reward 0.0\n",
      "done? False\n",
      "step 10 : Right\n",
      "  (Right)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "current state: 1\n",
      "reward 0.0\n",
      "done? False\n"
     ]
    }
   ],
   "source": [
    "max_step = 10\n",
    "np.random.seed(42)\n",
    "\n",
    "env = gym.make('FrozenLake-v0').unwrapped\n",
    "\n",
    "# do a random walk\n",
    "for i in range(max_step):\n",
    "    env.reset()\n",
    "    random_action = env.action_space.sample()\n",
    "    state_new, reward, done, info = env.step(random_action)\n",
    "    print('step', i + 1, ':',['Left', 'Down', 'Right', 'Up'][random_action])\n",
    "    env.render()\n",
    "    print('current state:', state_new)\n",
    "    print('reward', reward)\n",
    "    print('done?', done)\n",
    "    if done: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CHPqSDaE_n2M"
   },
   "source": [
    "#### Q3. Q-Learning [50 points]\n",
    "\n",
    "You will ​implement​ ​Q-learning to solve the problem.​ Assume that the environment has states S and actions A. Use the ​function​ ​signature​ ​provided​ below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hgFa7qID_n2O",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The pseudocode for Q-Learning was described in lecture, but for your reference, we provide it here:\n",
    "![alt text](https://cs-people.bu.edu/sunxm/q-learning.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(env, alpha=0.5, gamma=0.95, epsilon=0.1, num_episodes=500, RAND = False):\n",
    "       \n",
    "    \"\"\" Performs Q-learning for the given environment.\n",
    "    Initialize Q to all zeros\n",
    "    env: Unwrapped OpenAI gym environment.\n",
    "    alpha: Learning rate parameter.\n",
    "    gamma: Decay rate (future reward discount) parameter.\n",
    "    num_episodes: Number of episodes to use for learning. \n",
    "    return: Q table, i.e. a table with the Q value for every <S, A> pair.\n",
    "    \"\"\" \n",
    "    np.random.seed(42)\n",
    "    state_num = env.nS\n",
    "    action_num = env.nA\n",
    "    Q = np.zeros((state_num, action_num))\n",
    "    \n",
    "    if RAND == True:\n",
    "        Q = np.random.rand(state_num, action_num)\n",
    "        terminal_state = [5, 7, 11, 12, 15]\n",
    "        Q[terminal_state, :] = 0\n",
    "    \n",
    "    for epis in range(num_episodes):                       # loop through episodes\n",
    "        \n",
    "        cur_state = env.reset()\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            if np.random.rand() < 1 - epsilon:\n",
    "                action = np.argmax(Q[cur_state, :])\n",
    "            else:\n",
    "                action = np.random.randint(0, action_num)\n",
    "            next_state, reward, done, _ = env.step(action)  \n",
    "            Q[cur_state, action] += alpha*(reward + gamma*np.argmax(Q[next_state,:]) - Q[cur_state, action])\n",
    "            cur_state = next_state\n",
    "            \n",
    "    return Q\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1JHa8uv6_n2N"
   },
   "source": [
    "#### Q4. Main Function [20 points]\n",
    "You also need to implement the main function to solve the entire FrozenLake-v0 problem, including setting up the Gym environment, \n",
    "calling the q-learning function as defined above, printing out the returned Q-table, etc. <br>\n",
    "\n",
    "You should use the $\\epsilon$-greedy algorithm (solving Exploration-Exploitation Dilemma) to generate actions for each state. Try `num_episodes` with different values, e.g. `num_episodes=500, 1000, 5000, 10000, 50000`. You should also try different initializations for the Q-table, i.e. Random Initialization and Zero Initialization. \n",
    "\n",
    "Please do not change any default value of environment setting, such as `is_slippery`, if not mentioned above.\n",
    "\n",
    "Provide​ ​the​ ​final​ ​Q-table​ ​of​ ​​FrozenLake-v0​ for **each <num_episode, init_method>** you have tried [10 points],​ ​and analyze ​what​ you observe [10 points]. \n",
    "\n",
    "If you are interested in testing your learned Q-learned, you can compute the win rate (the probability of agents to goal following the Q-table). Note that you should use `argmax` instead of epsilon greedy to choose the action at the current state. \n",
    "Win Rate is a good way to test whether you write your algorithm correctly.\n",
    "\n",
    "Just for your reference, we test our implementation for 10000 trials after 50000-episode training using zero initialization and the win rate is 0.4985. You should achieve similar value under the same test.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output of Q-learning\n",
      "Q:  [[2.3156105  2.75382185 2.69840237 2.85      ]\n",
      " [2.21593371 2.83687404 2.1554081  2.85      ]\n",
      " [2.1100158  2.66074217 2.67187359 2.85      ]\n",
      " [0.95620248 1.06664883 2.10340565 2.85      ]\n",
      " [0.48838856 0.52784219 0.95459414 0.19353756]\n",
      " [0.         0.         0.         0.        ]\n",
      " [1.81674138 0.02839981 0.01475556 0.16710326]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.48313145 0.65107202 0.03528444 0.83390001]\n",
      " [0.14484458 2.24216998 0.08075728 0.06787293]\n",
      " [0.86792496 0.52668685 0.01719426 0.02905188]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.11858592 0.48941271 1.1919105  0.11864013]\n",
      " [0.12404922 1.71171304 0.02261364 0.79066629]\n",
      " [0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "def winrate(Q, env, num_episodes):\n",
    "    num_win = 0.\n",
    "    \n",
    "    for epis in range(num_episodes):\n",
    "        \n",
    "        cur_state = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            \n",
    "            action = np.argmax(Q[cur_state, :])\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            num_win += reward\n",
    "            cur_state = next_state\n",
    "    \n",
    "    return num_win/num_episodes\n",
    "\n",
    "def main():\n",
    "    env = gym.make('FrozenLake-v0').unwrapped\n",
    "    np.random.seed(42)\n",
    "\n",
    "    print('output of Q-learning')\n",
    "    Q = q_learning(env, num_episodes = 500, RAND = True)\n",
    "    print('Q: ', Q)\n",
    "    print(winrate(Q, env, num_episodes = 500))\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the situation of zero-initialization, when we only have 500 or even 5000 episodes, the agent never goes to the goal in these tries and then it gets no non-zero rewards during the whole traing. After trying more, like 50000 episodes, the agent gets to the final goal and it gets non-zero rewards from the environment and then it begins to learn. Thus, when the number of episodes grows up to 500000, Q matrix has non-zero entries.\n",
    "\n",
    "In the situation of random-initialization, in the first 500 episodes, the agent gets to the final goal and starts learning, so the Q-values are non-zeros.\n",
    "\n",
    "Random initialization will help the agent easily gets to the final goal in the early episodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FtF6U2vg_n2O"
   },
   "source": [
    "#### Additional Instructions\n",
    "If​ ​you’re​ ​new​ ​to​ OpenAI’s​ ​Gym,​ first ​go​ ​through​ ​OpenAI’s​ ​full​ ​tutorial listed​ ​earlier​ ​and​ ​visit​ ​the​ ​Appendix​ ​to​ ​this​ ​homework​ ​before​ ​proceeding.\n",
    "Some additional rules:\n",
    "- Only submit **original code** written entirely by you.\n",
    "- **Permitted​​ non-standard ​​libraries**:​​ ​gym​,​​ ​numpy.\n",
    "- **Only​ ​use​ ​numpy​ ​for​ ​random​ ​sampling,​ ​and​ ​seed​ ​at​ ​the​ ​beginning​ ​of​ ​each​ ​function​ ​with**:\n",
    "np.random.seed(42)\n",
    "- **Unwrap​ ​the​ ​OpenAI​ ​gym​ ​before​ ​providing​ ​them​ ​to​ ​these​ ​functions.** <br>\n",
    "env​ ​=​ ​gym.make(“FrozenLake-v0”).unwrapped\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RjpJpTbV_n2Q"
   },
   "source": [
    "## Appendix\n",
    "\n",
    "This​ ​appendix​ ​includes​ ​references​ ​to​ ​APIs​ ​you​ ​may​ ​find​ ​useful.​ ​If​ ​the​ ​description​ ​sounds​ ​useful,​ ​check out\n",
    "the​ ​respective​ ​package’s​ ​documentation​ ​for​ ​a​ ​much​ ​better​ ​description​ ​than​ ​we​ ​could​ ​provide.\n",
    "\n",
    "#### Numpy\n",
    "- np.zeros:​ ​N-dimensional​ ​tensor​ ​initialized​ ​to​ ​all​ ​0s.\n",
    "- np.ones:​ ​N-dimensional​ ​tensor​ ​initialized​ ​to​ ​all​ ​1s.\n",
    "- np.eye:​ ​N-dimensional​ ​tensor​ ​initialized​ ​to​ ​a​ ​diagonal​ ​matrix​ ​of​ ​1s.\n",
    "- np.random.choice:​ ​Randomly​ ​sample​ ​from​ ​a​ ​list,​ ​allowing​ ​you​ ​to​ ​specify​ ​weights.\n",
    "- np.argmax:​ ​Index​ ​of​ ​the​ ​maximum​ ​element.\n",
    "- np.abs:​ ​Absolute​ ​value.\n",
    "- np.mean:​ ​Average​ ​across​ ​dimensions.\n",
    "- np.sum:​ ​Sum​ ​across​ ​dimensions.\n",
    "\n",
    "### OpenAI Gym\n",
    "- Environment​ ​(unwrapped):<br>\n",
    "env.nS #​ ​Number​ ​of​ ​spaces. <br>\n",
    "env.nA #​ ​Number​ ​of​ ​actions. <br>\n",
    "env.P #​ ​Dynamics​ ​model.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "pset7.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
