{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A9ZmxP6StAoz"
   },
   "source": [
    "# CS542 - Class Challenge - fine-grained classification of plants:\n",
    "\n",
    "Our class challenge will consists of two tasks addressing an image recognition task where our dataset contains about 1K categories of plants with only about 250,000 images.  There will be two parts to this task:\n",
    "\n",
    "1. Image classification. Imagine we have cateloged all the plants we care to identify, now we just need to create a classifier for them! Use your skills from the supervised learning sections of this course to try to address this problem.\n",
    "\n",
    "2. Semi-Supervised/Few-Shot Learning.  Unfortunately, we missed some important plants we want to classify!  We do have some images we think contain the plant, but we have only have a few labels.  Our new goal is to develop an AI model that can learn from just these labeled examples.\n",
    "\n",
    "Each student must submit a model on both tasks.  Students in the top 3 on each task will get 5% extra credit on this assignment.\n",
    "\n",
    "This notebook is associated with the second task (semi-supervised).\n",
    "\n",
    "\n",
    "# Dataset\n",
    "The dataset is downloaded on scc in the address: \"/projectnb2/cs542-bap/classChallenge/data\". You can find the python version of this notebook there as well or you could just type \"jupyter nbconvert --to script baselineModel_task2.ipynb\" and it will output \"baselineModel_task2.py\". You should be able to run \"baselineModel_task2.py\" on scc by simply typing \"python baselineModel_task2.py\"\n",
    "\n",
    "Please don't try to change or delete the dataset.\n",
    "\n",
    "# Evaluation:\n",
    "You will compete with each other over your performance on the dedicated test set. The performance measure is classification accuracy, i.e: if the true class is your top predictions. \n",
    "\n",
    "# Baseline:\n",
    "The following code is a baseline which you can use and improve to come up with your model for this task\n",
    "\n",
    "# Suggestion\n",
    "One simple suggestion would be to use a pretrained model on imagenet and finetune it on this data similar to this [link](https://keras.io/api/applications/)\n",
    "Also you should likely train more than 2 epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4q8oub7ntAo1"
   },
   "source": [
    "## Import TensorFlow and other libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "14D2EZ17tAo1"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import PIL\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BYaBUsR-tAo3"
   },
   "source": [
    "# Create a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "m893cNgztAo3"
   },
   "outputs": [],
   "source": [
    "data_dir = '/projectnb2/cs542-bap/class_challenge/'\n",
    "\n",
    "train_samps = np.loadtxt(os.path.join(data_dir, 'train_held_out_labeled.txt'), dtype='str', delimiter=\" \")\n",
    "val_samps = np.loadtxt(os.path.join(data_dir, 'val_held_out.txt'), dtype='str', delimiter=\" \")\n",
    "\n",
    "train_len = len(train_samps)\n",
    "val_len = len(val_samps)\n",
    "\n",
    "samples = np.concatenate((train_samps, val_samps))\n",
    "\n",
    "unlabeled_samps = np.loadtxt(os.path.join(data_dir, 'train_held_out.txt'), dtype='str')\n",
    "unlabeled_len = len(unlabeled_samps)\n",
    "\n",
    "test_ds = tf.data.TextLineDataset(os.path.join(data_dir, 'test_held_out.txt'))\n",
    "\n",
    "with open(os.path.join(data_dir, 'classes_held_out.txt'), 'r') as f:\n",
    "    class_names = [c.strip() for c in f.readlines()]\n",
    "\n",
    "num_classes = len(class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2m6getXwtAo3"
   },
   "source": [
    "## Write a short function that converts a file path to an (img, label) pair:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ZrIrN5iItAo3"
   },
   "outputs": [],
   "source": [
    "def decode_img(img, test=False, crop_size=224):\n",
    "    img = tf.io.read_file(img)\n",
    "    # convert the compressed string to a 3D uint8 tensor\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "\n",
    "    return tf.image.resize(img, [crop_size, crop_size])\n",
    "  \n",
    "def get_label(label):\n",
    "    # find teh matching label\n",
    "    one_hot = tf.where(tf.equal(label, class_names))\n",
    "    # Integer encode the label\n",
    "    return tf.reduce_min(one_hot)\n",
    "\n",
    "def process_path(path, label):\n",
    "    # should have two parts\n",
    "    # file_path = tf.strings.split(file_path)\n",
    "    # second part has the class index\n",
    "    label = get_label(label)\n",
    "   # load the raw data from the file\n",
    "    img = decode_img(tf.strings.join([data_dir, 'images/', path, '.jpg']))\n",
    "    return img, label\n",
    "\n",
    "def process_path_test(file_path):\n",
    "    # load the raw data from the file\n",
    "    img = decode_img(tf.strings.join([data_dir, 'images/', file_path, '.jpg']))\n",
    "    return img, file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "epSS5nlYtAo3"
   },
   "source": [
    "# Finish setting up data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "oIrk9SuYtAo3"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "in user code:\n\n\n    TypeError: tf__process_path_test() takes 1 positional argument but 2 were given\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-2272369ce1ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mAUTOTUNE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAUTOTUNE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtest_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_ds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_path_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_parallel_calls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mAUTOTUNE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mconfigure_for_performance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/pkg.7/tensorflow/2.3.1/install/lib/SCC/../python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, map_func, num_parallel_calls, deterministic)\u001b[0m\n\u001b[1;32m   1695\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mMapDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreserve_cardinality\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1696\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1697\u001b[0;31m       return ParallelMapDataset(\n\u001b[0m\u001b[1;32m   1698\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1699\u001b[0m           \u001b[0mmap_func\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/pkg.7/tensorflow/2.3.1/install/lib/SCC/../python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dataset, map_func, num_parallel_calls, deterministic, use_inter_op_parallelism, preserve_cardinality, use_legacy_function)\u001b[0m\n\u001b[1;32m   4078\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_input_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4079\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_use_inter_op_parallelism\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muse_inter_op_parallelism\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4080\u001b[0;31m     self._map_func = StructuredFunctionWrapper(\n\u001b[0m\u001b[1;32m   4081\u001b[0m         \u001b[0mmap_func\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4082\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transformation_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/pkg.7/tensorflow/2.3.1/install/lib/SCC/../python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[1;32m   3369\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mtracking\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresource_tracker_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_tracker\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3370\u001b[0m         \u001b[0;31m# TODO(b/141462134): Switch to using garbage collection.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3371\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrapper_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_concrete_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3372\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0madd_to_graph\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3373\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_to_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/pkg.7/tensorflow/2.3.1/install/lib/SCC/../python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mget_concrete_function\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2936\u001b[0m       \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0minputs\u001b[0m \u001b[0mto\u001b[0m \u001b[0mspecialize\u001b[0m \u001b[0mon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2937\u001b[0m     \"\"\"\n\u001b[0;32m-> 2938\u001b[0;31m     graph_function = self._get_concrete_function_garbage_collected(\n\u001b[0m\u001b[1;32m   2939\u001b[0m         *args, **kwargs)\n\u001b[1;32m   2940\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_garbage_collector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/pkg.7/tensorflow/2.3.1/install/lib/SCC/../python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2904\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2905\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2906\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2907\u001b[0m       \u001b[0mseen_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2908\u001b[0m       captured = object_identity.ObjectIdentitySet(\n",
      "\u001b[0;32m/share/pkg.7/tensorflow/2.3.1/install/lib/SCC/../python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3212\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3213\u001b[0;31m       \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3214\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3215\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/pkg.7/tensorflow/2.3.1/install/lib/SCC/../python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3063\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3064\u001b[0m     graph_function = ConcreteFunction(\n\u001b[0;32m-> 3065\u001b[0;31m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[1;32m   3066\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3067\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/pkg.7/tensorflow/2.3.1/install/lib/SCC/../python3.8/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 986\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    987\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/pkg.7/tensorflow/2.3.1/install/lib/SCC/../python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mwrapper_fn\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m   3362\u001b[0m           attributes=defun_kwargs)\n\u001b[1;32m   3363\u001b[0m       \u001b[0;32mdef\u001b[0m \u001b[0mwrapper_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=missing-docstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3364\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_wrapper_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3365\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3366\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/pkg.7/tensorflow/2.3.1/install/lib/SCC/../python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m_wrapper_helper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m   3297\u001b[0m         \u001b[0mnested_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3299\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mautograph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_convert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3300\u001b[0m       \u001b[0;31m# If `func` returns a list of tensors, `nest.flatten()` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3301\u001b[0m       \u001b[0;31m# `ops.convert_to_tensor()` would conspire to attempt to stack\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/share/pkg.7/tensorflow/2.3.1/install/lib/SCC/../python3.8/site-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    256\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ag_error_metadata'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m           \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m           \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: in user code:\n\n\n    TypeError: tf__process_path_test() takes 1 positional argument but 2 were given\n"
     ]
    }
   ],
   "source": [
    "batch_size = 25\n",
    "\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "test_ds = test_ds.map(process_path_test, num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "def configure_for_performance(ds):\n",
    "    ds = ds.cache()\n",
    "    ds = ds.shuffle(buffer_size=1000)\n",
    "    ds = ds.batch(batch_size)\n",
    "    ds = ds.prefetch(buffer_size=AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "def shuffle_train_val(train_perc = 0.15):\n",
    "    \"\"\"\n",
    "    This function returns shuffled train data and val data\n",
    "    The default is we take 20% samples as training data\n",
    "    \"\"\"\n",
    "    # define the train length\n",
    "    train_len = int(train_perc*len(samples))\n",
    "    \n",
    "    # idexing train set and val set by random choice\n",
    "    train_idx = np.random.choice(range(len(samples)), train_len, replace=True)\n",
    "    val_idx = [idx for idx in range(len(samples)) if idx not in train_idx]\n",
    "    \n",
    "    # get train_ds and val_ds based on indexes\n",
    "    train_ds = tf.data.Dataset.from_tensor_slices((samples[train_idx, 0], samples[train_idx, 1]))\n",
    "    train_ds = train_ds.map(process_path, num_parallel_calls=AUTOTUNE)\n",
    "    train_ds = configure_for_performance(train_ds)\n",
    "    val_ds = tf.data.Dataset.from_tensor_slices((samples[val_idx, 0], samples[val_idx, 1]))\n",
    "    val_ds = val_ds.map(process_path, num_parallel_calls=AUTOTUNE)\n",
    "    val_ds = configure_for_performance(val_ds)\n",
    "\n",
    "    return train_ds, val_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JyOA90IRtAo4"
   },
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "wWHpzQoatAo4"
   },
   "outputs": [],
   "source": [
    "class ResNet50(tf.keras.Model):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(ResNet50, self).__init__()\n",
    "        self.ResNet50 = keras.applications.ResNet50(\n",
    "            include_top=False,\n",
    "            weights='imagenet',\n",
    "            input_shape=(224, 224, 3)\n",
    "        )\n",
    "        \n",
    "        # unfreeze the last two layers\n",
    "        for layer in self.ResNet50.layers[:-2]:\n",
    "            layer.trainable = False\n",
    "        \n",
    "        # define layers\n",
    "        self.pool = layers.GlobalAveragePooling2D()\n",
    "        self.flatten = layers.Flatten()\n",
    "        self.fc_1 = layers.Dense(1024)\n",
    "        self.fc_2 = layers.Dense(units=num_classes)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = keras.applications.resnet.preprocess_input(inputs)\n",
    "        x = self.ResNet50(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc_1(x)\n",
    "        output = self.fc_2(x)\n",
    "\n",
    "        return output\n",
    "\n",
    "# data augmentation\n",
    "model = Sequential([\n",
    "    layers.experimental.preprocessing.RandomFlip(\n",
    "        mode='horizontal'),\n",
    "    layers.experimental.preprocessing.RandomZoom(0.2),\n",
    "    layers.experimental.preprocessing.RandomTranslation(0.2, 0.2),\n",
    "    ResNet50()\n",
    "])\n",
    "\n",
    "# compile the model\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.00001),\n",
    "    loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EfficientB0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EfficientB0(tf.keras.Model):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(EfficientB0, self).__init__()\n",
    "        self.EfficientB0 = keras.applications.EfficientNetB0(\n",
    "             include_top=False,\n",
    "             weights='imagenet',\n",
    "             input_shape=(224, 224, 3), \n",
    "             # add stronger reguarliztions\n",
    "             drop_connect_rate=0.4\n",
    "        )\n",
    "        \n",
    "        # unfreeze top 20 layers\n",
    "        for layer in self.EfficientB0.layers[:-20]:\n",
    "            layer.trainable = False\n",
    "            \n",
    "        # define layers\n",
    "        self.pool = layers.GlobalAveragePooling2D()\n",
    "        self.flatten = layers.Flatten()\n",
    "        self.fc_1 = layers.Dense(1024)\n",
    "        self.dropout = layers.Dropout(0.3)\n",
    "        self.fc_2 = layers.Dense(units=num_classes)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.EfficientB0(inputs)\n",
    "      # x = self.flatten(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.fc_1(x)\n",
    "        x = self.dropout(x)\n",
    "        output = self.fc_2(x)\n",
    "\n",
    "        return output\n",
    "\n",
    "# image augmentation\n",
    "model = Sequential([\n",
    "    layers.experimental.preprocessing.RandomFlip(\n",
    "       mode='horizontal'),\n",
    "    layers.experimental.preprocessing.RandomZoom(0.2),\n",
    "    layers.experimental.preprocessing.RandomTranslation(0.2, 0.2),\n",
    "    EfficientB0()\n",
    "])\n",
    "\n",
    "# compile the model\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.00001),\n",
    "    loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Add_labels(unlabeled_samps, model, unlabeled_batch):\n",
    "    \"\"\"\n",
    "    unlabeled_samps: the indexes of unlabeled samples\n",
    "    unlabeled_batch: the number of unlabelled data to be predicted\n",
    "    return: a bunch of predcitions of unlabeled samples\n",
    "    \"\"\"\n",
    "    # decode the unlabelled images\n",
    "    unlabeled_ds = tf.data.Dataset.from_tensor_slices(unlabeled_samps)\n",
    "    unlabeled_ds = unlabeled_ds.map(process_path_test, num_parallel_calls=AUTOTUNE)\n",
    "    unlabeled_ds = unlabeled_ds.batch(1)\n",
    "    \n",
    "    # initialize prediction tracker\n",
    "    predictions = None\n",
    "    # initialize indexes tracker\n",
    "    inds = []\n",
    "    for image, image_name in unlabeled_ds:\n",
    "        preds = model.predict(image)\n",
    "        ind = np.argmax(preds)\n",
    "        cls = class_names[ind]\n",
    "        pred = (str(int(image_name)), cls)\n",
    "        \n",
    "        # keep tracking predictions\n",
    "        if predictions is None:\n",
    "            predictions = np.array(pred)\n",
    "        else:\n",
    "            predictions = np.vstack((predictions, pred))\n",
    "            \n",
    "        # keep tracking the indexes\n",
    "        inds.append(preds[0, ind])\n",
    "        \n",
    "    # output top n predictions, n = max_unlabeled\n",
    "    inds = np.argpartition(inds, -unlabeled_batch)[-unlabeled_batch:]\n",
    "    predictions = predictions[inds]\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1\n",
      "Epoch 1/5\n",
      "6/6 [==============================] - 5s 827ms/step - loss: 3.1277 - accuracy: 0.0355 - val_loss: 3.1090 - val_accuracy: 0.0378\n",
      "Epoch 2/5\n",
      "6/6 [==============================] - 3s 430ms/step - loss: 3.0973 - accuracy: 0.0426 - val_loss: 3.0657 - val_accuracy: 0.0395\n",
      "Epoch 3/5\n",
      "6/6 [==============================] - 3s 437ms/step - loss: 3.0471 - accuracy: 0.0780 - val_loss: 3.0255 - val_accuracy: 0.0533\n",
      "Epoch 4/5\n",
      "6/6 [==============================] - 3s 421ms/step - loss: 3.0048 - accuracy: 0.0638 - val_loss: 2.9860 - val_accuracy: 0.0670\n",
      "Epoch 5/5\n",
      "6/6 [==============================] - 3s 418ms/step - loss: 2.9737 - accuracy: 0.1135 - val_loss: 2.9461 - val_accuracy: 0.0808\n",
      "Epoch 1/5\n",
      "6/6 [==============================] - 2s 415ms/step - loss: 2.9257 - accuracy: 0.1135 - val_loss: 2.9087 - val_accuracy: 0.0962\n",
      "Epoch 2/5\n",
      "6/6 [==============================] - 2s 416ms/step - loss: 2.8819 - accuracy: 0.1277 - val_loss: 2.8717 - val_accuracy: 0.1271\n",
      "Epoch 3/5\n",
      "6/6 [==============================] - 2s 415ms/step - loss: 2.8175 - accuracy: 0.1631 - val_loss: 2.8357 - val_accuracy: 0.1649\n",
      "Epoch 4/5\n",
      "6/6 [==============================] - 3s 417ms/step - loss: 2.7573 - accuracy: 0.1773 - val_loss: 2.8003 - val_accuracy: 0.1787\n",
      "Epoch 5/5\n",
      "6/6 [==============================] - 2s 414ms/step - loss: 2.7864 - accuracy: 0.1844 - val_loss: 2.7656 - val_accuracy: 0.2079\n",
      "Epoch 1/5\n",
      "6/6 [==============================] - 2s 415ms/step - loss: 2.7035 - accuracy: 0.1986 - val_loss: 2.7302 - val_accuracy: 0.2285\n",
      "Epoch 2/5\n",
      "6/6 [==============================] - 3s 419ms/step - loss: 2.6812 - accuracy: 0.2199 - val_loss: 2.6964 - val_accuracy: 0.2423\n",
      "Epoch 3/5\n",
      "6/6 [==============================] - 3s 420ms/step - loss: 2.6034 - accuracy: 0.2482 - val_loss: 2.6629 - val_accuracy: 0.2646\n",
      "Epoch 4/5\n",
      "6/6 [==============================] - 3s 422ms/step - loss: 2.6173 - accuracy: 0.2979 - val_loss: 2.6304 - val_accuracy: 0.2973\n",
      "Epoch 5/5\n",
      "6/6 [==============================] - 3s 424ms/step - loss: 2.5554 - accuracy: 0.2482 - val_loss: 2.5980 - val_accuracy: 0.3162\n",
      "Epoch 1/5\n",
      "6/6 [==============================] - 2s 415ms/step - loss: 2.5045 - accuracy: 0.2979 - val_loss: 2.5680 - val_accuracy: 0.3265\n",
      "Epoch 2/5\n",
      "6/6 [==============================] - 3s 420ms/step - loss: 2.4555 - accuracy: 0.3050 - val_loss: 2.5371 - val_accuracy: 0.3471\n",
      "Epoch 3/5\n",
      "6/6 [==============================] - 3s 421ms/step - loss: 2.4464 - accuracy: 0.3333 - val_loss: 2.5062 - val_accuracy: 0.3574\n",
      "Epoch 4/5\n",
      "6/6 [==============================] - 2s 414ms/step - loss: 2.4056 - accuracy: 0.3475 - val_loss: 2.4749 - val_accuracy: 0.3746\n",
      "Epoch 5/5\n",
      "6/6 [==============================] - 3s 417ms/step - loss: 2.3641 - accuracy: 0.3404 - val_loss: 2.4430 - val_accuracy: 0.3814\n",
      "number of unlabeled samples remained: 3788\n",
      "Epoch 1/5\n",
      "6/6 [==============================] - 3s 419ms/step - loss: 2.3537 - accuracy: 0.3830 - val_loss: 2.4115 - val_accuracy: 0.3969\n",
      "Epoch 2/5\n",
      "6/6 [==============================] - 3s 419ms/step - loss: 2.2525 - accuracy: 0.4468 - val_loss: 2.3827 - val_accuracy: 0.4141\n",
      "Epoch 3/5\n",
      "6/6 [==============================] - 3s 417ms/step - loss: 2.2105 - accuracy: 0.4610 - val_loss: 2.3539 - val_accuracy: 0.4210\n",
      "Epoch 4/5\n",
      "6/6 [==============================] - 3s 421ms/step - loss: 2.2328 - accuracy: 0.4255 - val_loss: 2.3247 - val_accuracy: 0.4261\n",
      "Epoch 5/5\n",
      "6/6 [==============================] - 2s 414ms/step - loss: 2.1586 - accuracy: 0.4184 - val_loss: 2.2954 - val_accuracy: 0.4416\n",
      "Epoch 1/5\n",
      "6/6 [==============================] - 3s 418ms/step - loss: 2.1333 - accuracy: 0.4539 - val_loss: 2.2665 - val_accuracy: 0.4536\n",
      "Epoch 2/5\n",
      "6/6 [==============================] - 3s 417ms/step - loss: 2.1783 - accuracy: 0.4539 - val_loss: 2.2409 - val_accuracy: 0.4674\n",
      "Epoch 3/5\n",
      "6/6 [==============================] - 2s 413ms/step - loss: 2.0977 - accuracy: 0.4965 - val_loss: 2.2160 - val_accuracy: 0.4742\n",
      "Epoch 4/5\n",
      "6/6 [==============================] - 3s 417ms/step - loss: 2.0531 - accuracy: 0.5390 - val_loss: 2.1925 - val_accuracy: 0.4811\n",
      "Epoch 5/5\n",
      "6/6 [==============================] - 2s 416ms/step - loss: 2.0463 - accuracy: 0.5248 - val_loss: 2.1687 - val_accuracy: 0.5000\n",
      "Epoch 1/5\n",
      "6/6 [==============================] - 2s 416ms/step - loss: 2.0080 - accuracy: 0.5603 - val_loss: 2.1443 - val_accuracy: 0.5052\n",
      "Epoch 2/5\n",
      "6/6 [==============================] - 3s 417ms/step - loss: 1.9849 - accuracy: 0.5390 - val_loss: 2.1210 - val_accuracy: 0.5086\n",
      "Epoch 3/5\n",
      "6/6 [==============================] - 2s 415ms/step - loss: 1.9363 - accuracy: 0.5745 - val_loss: 2.0966 - val_accuracy: 0.5206\n",
      "Epoch 4/5\n",
      "6/6 [==============================] - 2s 416ms/step - loss: 1.9519 - accuracy: 0.5957 - val_loss: 2.0742 - val_accuracy: 0.5258\n",
      "Epoch 5/5\n",
      "6/6 [==============================] - 3s 424ms/step - loss: 1.8486 - accuracy: 0.6099 - val_loss: 2.0531 - val_accuracy: 0.5309\n",
      "number of unlabeled samples remained: 3368\n",
      "Epoch 1/5\n",
      "6/6 [==============================] - 3s 420ms/step - loss: 1.8891 - accuracy: 0.6099 - val_loss: 2.0326 - val_accuracy: 0.5344\n",
      "Epoch 2/5\n",
      "6/6 [==============================] - 3s 417ms/step - loss: 1.8109 - accuracy: 0.6383 - val_loss: 2.0122 - val_accuracy: 0.5395\n",
      "Epoch 3/5\n",
      "6/6 [==============================] - 3s 417ms/step - loss: 1.8193 - accuracy: 0.6596 - val_loss: 1.9915 - val_accuracy: 0.5498\n",
      "Epoch 4/5\n",
      "6/6 [==============================] - 2s 414ms/step - loss: 1.7839 - accuracy: 0.6383 - val_loss: 1.9722 - val_accuracy: 0.5447\n",
      "Epoch 5/5\n",
      "6/6 [==============================] - 3s 418ms/step - loss: 1.7586 - accuracy: 0.6596 - val_loss: 1.9539 - val_accuracy: 0.5498\n",
      "number of unlabeled samples remained: 2948\n",
      "Epoch 1/5\n",
      "6/6 [==============================] - 3s 420ms/step - loss: 1.7001 - accuracy: 0.6738 - val_loss: 1.9368 - val_accuracy: 0.5533\n",
      "Epoch 2/5\n",
      "6/6 [==============================] - 2s 416ms/step - loss: 1.7007 - accuracy: 0.6738 - val_loss: 1.9193 - val_accuracy: 0.5601\n",
      "Epoch 3/5\n",
      "6/6 [==============================] - 2s 416ms/step - loss: 1.7008 - accuracy: 0.6525 - val_loss: 1.9029 - val_accuracy: 0.5687\n",
      "Epoch 4/5\n",
      "6/6 [==============================] - 3s 417ms/step - loss: 1.6905 - accuracy: 0.6454 - val_loss: 1.8869 - val_accuracy: 0.5687\n",
      "Epoch 5/5\n",
      "6/6 [==============================] - 3s 417ms/step - loss: 1.6596 - accuracy: 0.6667 - val_loss: 1.8681 - val_accuracy: 0.5704\n",
      "number of unlabeled samples remained: 2528\n",
      "Epoch 1/5\n",
      "6/6 [==============================] - 3s 423ms/step - loss: 1.6595 - accuracy: 0.6667 - val_loss: 1.8518 - val_accuracy: 0.5722\n",
      "Epoch 2/5\n",
      "6/6 [==============================] - 2s 415ms/step - loss: 1.6556 - accuracy: 0.6738 - val_loss: 1.8360 - val_accuracy: 0.5773\n",
      "Epoch 3/5\n",
      "6/6 [==============================] - 2s 416ms/step - loss: 1.5290 - accuracy: 0.7660 - val_loss: 1.8204 - val_accuracy: 0.5790\n",
      "Epoch 4/5\n",
      "6/6 [==============================] - 3s 423ms/step - loss: 1.5565 - accuracy: 0.7305 - val_loss: 1.8070 - val_accuracy: 0.5825\n",
      "Epoch 5/5\n",
      "6/6 [==============================] - 3s 420ms/step - loss: 1.5362 - accuracy: 0.6738 - val_loss: 1.7934 - val_accuracy: 0.5859\n",
      "number of unlabeled samples remained: 2108\n",
      "Epoch 1/5\n",
      "6/6 [==============================] - 3s 419ms/step - loss: 1.5188 - accuracy: 0.6950 - val_loss: 1.7794 - val_accuracy: 0.5859\n",
      "Epoch 2/5\n",
      "6/6 [==============================] - 2s 416ms/step - loss: 1.5174 - accuracy: 0.7447 - val_loss: 1.7660 - val_accuracy: 0.5876\n",
      "Epoch 3/5\n",
      "6/6 [==============================] - 2s 414ms/step - loss: 1.4635 - accuracy: 0.7305 - val_loss: 1.7534 - val_accuracy: 0.5893\n",
      "Epoch 4/5\n",
      "6/6 [==============================] - 2s 416ms/step - loss: 1.4686 - accuracy: 0.7234 - val_loss: 1.7406 - val_accuracy: 0.5945\n",
      "Epoch 5/5\n",
      "6/6 [==============================] - 2s 417ms/step - loss: 1.5278 - accuracy: 0.6738 - val_loss: 1.7272 - val_accuracy: 0.5962\n",
      "number of unlabeled samples remained: 1688\n",
      "Epoch 1/5\n",
      "6/6 [==============================] - 3s 419ms/step - loss: 1.3897 - accuracy: 0.7305 - val_loss: 1.7148 - val_accuracy: 0.5979\n",
      "Epoch 2/5\n",
      "6/6 [==============================] - 2s 416ms/step - loss: 1.4280 - accuracy: 0.7447 - val_loss: 1.7031 - val_accuracy: 0.5997\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5\n",
      "6/6 [==============================] - 3s 419ms/step - loss: 1.3944 - accuracy: 0.7163 - val_loss: 1.6908 - val_accuracy: 0.6048\n",
      "Epoch 4/5\n",
      "6/6 [==============================] - 3s 418ms/step - loss: 1.3833 - accuracy: 0.7589 - val_loss: 1.6784 - val_accuracy: 0.6065\n",
      "Epoch 5/5\n",
      "6/6 [==============================] - 2s 416ms/step - loss: 1.3800 - accuracy: 0.7376 - val_loss: 1.6680 - val_accuracy: 0.6100\n",
      "number of unlabeled samples remained: 1268\n",
      "Epoch 1/5\n",
      "6/6 [==============================] - 3s 422ms/step - loss: 1.3970 - accuracy: 0.7163 - val_loss: 1.6570 - val_accuracy: 0.6117\n",
      "Epoch 2/5\n",
      "6/6 [==============================] - 2s 414ms/step - loss: 1.3384 - accuracy: 0.7660 - val_loss: 1.6456 - val_accuracy: 0.6134\n",
      "Epoch 3/5\n",
      "6/6 [==============================] - 2s 415ms/step - loss: 1.3754 - accuracy: 0.7234 - val_loss: 1.6348 - val_accuracy: 0.6134\n",
      "Epoch 4/5\n",
      "6/6 [==============================] - 3s 417ms/step - loss: 1.3432 - accuracy: 0.7730 - val_loss: 1.6253 - val_accuracy: 0.6134\n",
      "Epoch 5/5\n",
      "6/6 [==============================] - 3s 418ms/step - loss: 1.3603 - accuracy: 0.7376 - val_loss: 1.6166 - val_accuracy: 0.6168\n",
      "number of unlabeled samples remained: 848\n",
      "Epoch 1/5\n",
      "6/6 [==============================] - 3s 423ms/step - loss: 1.2717 - accuracy: 0.7872 - val_loss: 1.6055 - val_accuracy: 0.6186\n",
      "Epoch 2/5\n",
      "6/6 [==============================] - 2s 414ms/step - loss: 1.2899 - accuracy: 0.8014 - val_loss: 1.5949 - val_accuracy: 0.6186\n",
      "Epoch 3/5\n",
      "6/6 [==============================] - 2s 417ms/step - loss: 1.2896 - accuracy: 0.7447 - val_loss: 1.5862 - val_accuracy: 0.6203\n",
      "Epoch 4/5\n",
      "6/6 [==============================] - 2s 417ms/step - loss: 1.2586 - accuracy: 0.7660 - val_loss: 1.5750 - val_accuracy: 0.6220\n",
      "Epoch 5/5\n",
      "6/6 [==============================] - 2s 416ms/step - loss: 1.2553 - accuracy: 0.7730 - val_loss: 1.5656 - val_accuracy: 0.6254\n",
      "number of unlabeled samples remained: 428\n",
      "Epoch 1/5\n",
      "6/6 [==============================] - 2s 416ms/step - loss: 1.2362 - accuracy: 0.7801 - val_loss: 1.5559 - val_accuracy: 0.6254\n",
      "Epoch 2/5\n",
      "6/6 [==============================] - 3s 417ms/step - loss: 1.2489 - accuracy: 0.7801 - val_loss: 1.5462 - val_accuracy: 0.6271\n",
      "Epoch 3/5\n",
      "6/6 [==============================] - 2s 416ms/step - loss: 1.1715 - accuracy: 0.7943 - val_loss: 1.5379 - val_accuracy: 0.6289\n",
      "Epoch 4/5\n",
      "6/6 [==============================] - 3s 417ms/step - loss: 1.1978 - accuracy: 0.7660 - val_loss: 1.5292 - val_accuracy: 0.6323\n",
      "Epoch 5/5\n",
      "6/6 [==============================] - 3s 418ms/step - loss: 1.1982 - accuracy: 0.7730 - val_loss: 1.5203 - val_accuracy: 0.6340\n",
      "number of unlabeled samples remained: 8\n",
      "Epoch 1/5\n",
      "6/6 [==============================] - 3s 420ms/step - loss: 1.1668 - accuracy: 0.8014 - val_loss: 1.5125 - val_accuracy: 0.6340\n",
      "Epoch 2/5\n",
      "6/6 [==============================] - 2s 415ms/step - loss: 1.1466 - accuracy: 0.8014 - val_loss: 1.5045 - val_accuracy: 0.6357\n",
      "Epoch 3/5\n",
      "6/6 [==============================] - 3s 417ms/step - loss: 1.1173 - accuracy: 0.8014 - val_loss: 1.4968 - val_accuracy: 0.6340\n",
      "Epoch 4/5\n",
      "6/6 [==============================] - 2s 416ms/step - loss: 1.1076 - accuracy: 0.8227 - val_loss: 1.4894 - val_accuracy: 0.6340\n",
      "Epoch 5/5\n",
      "6/6 [==============================] - 2s 414ms/step - loss: 1.1450 - accuracy: 0.8227 - val_loss: 1.4806 - val_accuracy: 0.6357\n",
      "number of unlabeled samples remained: 0\n",
      "fine tuning the model (iteration 1)\n",
      "Epoch 1/20\n",
      "6/6 [==============================] - 3s 421ms/step - loss: 1.0389 - accuracy: 0.8298 - val_loss: 1.4714 - val_accuracy: 0.6375\n",
      "Epoch 2/20\n",
      "6/6 [==============================] - 3s 420ms/step - loss: 1.0767 - accuracy: 0.7943 - val_loss: 1.4630 - val_accuracy: 0.6375\n",
      "Epoch 3/20\n",
      "6/6 [==============================] - 3s 417ms/step - loss: 1.0965 - accuracy: 0.8014 - val_loss: 1.4544 - val_accuracy: 0.6392\n",
      "Epoch 4/20\n",
      "6/6 [==============================] - 2s 416ms/step - loss: 1.0107 - accuracy: 0.8582 - val_loss: 1.4466 - val_accuracy: 0.6426\n",
      "Epoch 5/20\n",
      "6/6 [==============================] - 3s 417ms/step - loss: 1.0243 - accuracy: 0.8156 - val_loss: 1.4393 - val_accuracy: 0.6426\n",
      "Epoch 6/20\n",
      "6/6 [==============================] - 2s 416ms/step - loss: 0.9883 - accuracy: 0.8936 - val_loss: 1.4336 - val_accuracy: 0.6409\n",
      "Epoch 7/20\n",
      "6/6 [==============================] - 3s 417ms/step - loss: 1.0549 - accuracy: 0.7943 - val_loss: 1.4268 - val_accuracy: 0.6426\n",
      "Epoch 8/20\n",
      "6/6 [==============================] - 2s 413ms/step - loss: 1.0255 - accuracy: 0.7801 - val_loss: 1.4205 - val_accuracy: 0.6409\n",
      "Epoch 9/20\n",
      "6/6 [==============================] - 2s 413ms/step - loss: 0.9424 - accuracy: 0.8582 - val_loss: 1.4127 - val_accuracy: 0.6409\n",
      "Epoch 10/20\n",
      "6/6 [==============================] - 2s 416ms/step - loss: 0.9537 - accuracy: 0.8369 - val_loss: 1.4048 - val_accuracy: 0.6426\n",
      "Epoch 11/20\n",
      "6/6 [==============================] - 2s 414ms/step - loss: 0.9609 - accuracy: 0.8440 - val_loss: 1.3981 - val_accuracy: 0.6426\n",
      "Epoch 12/20\n",
      "6/6 [==============================] - 3s 418ms/step - loss: 0.9340 - accuracy: 0.8085 - val_loss: 1.3913 - val_accuracy: 0.6443\n",
      "Epoch 13/20\n",
      "6/6 [==============================] - 2s 413ms/step - loss: 0.9197 - accuracy: 0.8440 - val_loss: 1.3848 - val_accuracy: 0.6478\n",
      "Epoch 14/20\n",
      "6/6 [==============================] - 2s 416ms/step - loss: 0.8981 - accuracy: 0.8723 - val_loss: 1.3793 - val_accuracy: 0.6512\n",
      "Epoch 15/20\n",
      "6/6 [==============================] - 2s 416ms/step - loss: 0.9456 - accuracy: 0.8652 - val_loss: 1.3743 - val_accuracy: 0.6512\n",
      "Epoch 16/20\n",
      "6/6 [==============================] - 2s 415ms/step - loss: 0.9662 - accuracy: 0.8227 - val_loss: 1.3673 - val_accuracy: 0.6529\n",
      "Epoch 17/20\n",
      "6/6 [==============================] - 2s 414ms/step - loss: 0.9182 - accuracy: 0.8582 - val_loss: 1.3606 - val_accuracy: 0.6529\n",
      "Epoch 18/20\n",
      "6/6 [==============================] - 2s 416ms/step - loss: 0.8980 - accuracy: 0.8156 - val_loss: 1.3549 - val_accuracy: 0.6546\n",
      "Epoch 19/20\n",
      "6/6 [==============================] - 3s 417ms/step - loss: 0.9152 - accuracy: 0.8794 - val_loss: 1.3485 - val_accuracy: 0.6581\n",
      "Epoch 20/20\n",
      "6/6 [==============================] - 2s 415ms/step - loss: 0.8896 - accuracy: 0.8440 - val_loss: 1.3430 - val_accuracy: 0.6598\n",
      "Iteration 2\n",
      "Epoch 1/5\n",
      "6/6 [==============================] - 3s 450ms/step - loss: 1.4520 - accuracy: 0.6454 - val_loss: 1.2195 - val_accuracy: 0.7031\n",
      "Epoch 2/5\n",
      "6/6 [==============================] - 3s 421ms/step - loss: 1.4100 - accuracy: 0.6454 - val_loss: 1.2087 - val_accuracy: 0.7083\n",
      "Epoch 3/5\n",
      "6/6 [==============================] - 3s 417ms/step - loss: 1.3992 - accuracy: 0.6667 - val_loss: 1.1975 - val_accuracy: 0.7101\n",
      "Epoch 4/5\n",
      "6/6 [==============================] - 2s 416ms/step - loss: 1.4479 - accuracy: 0.6170 - val_loss: 1.1868 - val_accuracy: 0.7101\n",
      "Epoch 5/5\n",
      "6/6 [==============================] - 2s 412ms/step - loss: 1.3767 - accuracy: 0.6667 - val_loss: 1.1770 - val_accuracy: 0.7135\n",
      "number of unlabeled samples remained: 3788\n",
      "Epoch 1/5\n",
      "6/6 [==============================] - 2s 416ms/step - loss: 1.4185 - accuracy: 0.6596 - val_loss: 1.1675 - val_accuracy: 0.7118\n",
      "Epoch 2/5\n",
      "6/6 [==============================] - 2s 413ms/step - loss: 1.3521 - accuracy: 0.6525 - val_loss: 1.1586 - val_accuracy: 0.7118\n",
      "Epoch 3/5\n",
      "6/6 [==============================] - 2s 413ms/step - loss: 1.2884 - accuracy: 0.6950 - val_loss: 1.1499 - val_accuracy: 0.7118\n",
      "Epoch 4/5\n",
      "6/6 [==============================] - 2s 411ms/step - loss: 1.2326 - accuracy: 0.7092 - val_loss: 1.1418 - val_accuracy: 0.7101\n",
      "Epoch 5/5\n",
      "6/6 [==============================] - 2s 413ms/step - loss: 1.2801 - accuracy: 0.6667 - val_loss: 1.1341 - val_accuracy: 0.7118\n"
     ]
    }
   ],
   "source": [
    "model_list = [None] * 10\n",
    "    \n",
    "# the main training loop\n",
    "for i in range(10):\n",
    "  \n",
    "    model = model\n",
    "    train_ds, val_ds = shuffle_train_val()\n",
    "    samps = samples\n",
    "    unlabeled = unlabeled_samps\n",
    "\n",
    "    print(f\"Iteration {i+1}\")\n",
    "    unlabeled_batch = int(0.1 * unlabeled_len)\n",
    "    \n",
    "    # finish training this iteration until all unlabeled data are used\n",
    "    while len(unlabeled) > 0:\n",
    "        hist = model.fit(train_ds, validation_data=val_ds, epochs=5, shuffle=True)\n",
    "        improvement = hist.history['val_accuracy'][-1] - hist.history['val_accuracy'][-2]\n",
    "            \n",
    "        # as long as the model stop moving forward, start training unlabeled samples\n",
    "        if improvement <= 0.01:\n",
    "            preds = Add_labels(unlabeled, model, min(len(unlabeled), unlabeled_batch))\n",
    "            pred_ds = tf.data.Dataset.from_tensor_slices((preds[:,0], preds[:,1]))\n",
    "            pred_ds = pred_ds.map(process_path, num_parallel_calls=AUTOTUNE)\n",
    "            pred_ds = configure_for_performance(pred_ds)\n",
    "      \n",
    "            # keep updating the training set and the unlabeled set\n",
    "            train_ds.concatenate(pred_ds)\n",
    "            unlabeled = [j for j in unlabeled if j not in preds[:,0]]\n",
    "            print(f\"number of unlabeled samples remained: {len(unlabeled)}\")\n",
    "           \n",
    "    # train all labeled and unlabeled data\n",
    "    print(f\"fine tuning the model (iteration {i+1})\")\n",
    "    model.fit(train_ds,validation_data=val_ds,epochs=20,shuffle=True)\n",
    "        \n",
    "    # keep track of the trained models\n",
    "    model_list[i] = model\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist2 = np.load('hist2.npy',allow_pickle='TRUE').item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist1 = np.load('hist1.npy',allow_pickle='TRUE').item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# summarize history for accuracy\n",
    "plt.plot(hist.history['val_accuracy'])\n",
    "plt.plot(hist1['val_accuracy'])\n",
    "plt.plot(hist2['val_accuracy'])\n",
    "plt.plot(hist3['val_accuracy'])\n",
    "plt.plot(hist4['val_accuracy'])\n",
    "plt.plot(hist5['val_accuracy'])\n",
    "plt.title('validation accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('last 10 epochs')\n",
    "plt.ylim(0.5,1)\n",
    "plt.legend(['Basic EfficietNet','iteration 1','iteration 2','iteration 3','iteration 4','iteration 5'], \n",
    "           loc='center right', \n",
    "           bbox_to_anchor=(1.4, 0.5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VpPs4SWetAo4"
   },
   "source": [
    "# Output submission csv for Kaggle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = test_ds.batch(1)\n",
    "\n",
    "with open('submission_task2_semisupervised.csv', 'w') as f:\n",
    "  f.write('id,predicted\\n')\n",
    "  for image_batch, image_names in test_ds:\n",
    "    predictions = model_list[-1].predict(image_batch)\n",
    "    for image_name, predictions in zip(image_names.numpy(), model.predict(image_batch)):\n",
    "      inds = np.argmax(predictions)\n",
    "      line = str(int(image_name)) + ',' + class_names[inds]\n",
    "      f.write(line + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xt7OaOehvoh1"
   },
   "source": [
    "**Note**\n",
    "\n",
    "Absolute path is recommended here. For example, use \"/projectnb2/cs542-bap/[your directory name]/submission_task2_supervised.csv\" to replace \"submission_task2_supervised.csv\".\n",
    "\n",
    "Besides, you can request good resources by specify the type of gpus, such as \"qsub -l gpus=1 -l gpu_type=P100 [your file name].qsub\". This is helpful to avoid potential issues of GPUs, such as out of memory, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "baselineModel.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
